“中继适配器”模块将作为后端核心调度单元，负责解析用户意图并协调各模型执行。其设计可借鉴 HuggingGPT 等多模型协调框架，将流程分为**任务规划、模型选择、任务执行、响应整合**等阶段[medium.com](https://medium.com/@suvasism/llm-collaboration-tool-32a2a7185cd1#:~:text=Hugging GPT’s process into four,key stages)。主要功能模块可设计如下：

1. **请求解析**：接收来自前端的用户输入（对话指令或描述），解析其中的意图和需求。可以通过规则分析（例如关键词匹配“动作”、“音乐”、“场景”等）或引入小型 NLP 模型来识别涉及的子任务类型和参数。必要时，可将复杂请求拆解为多个子任务（例如“一段森林场景中的舞蹈动画配音乐”可拆解为“生成森林环境”+“生成舞蹈动作”+“生成背景音乐”三个子任务）。这一阶段的输出是结构化的任务列表，包括每个子任务的类型、使用模型、输入参数等。
2. **模型选择与任务拆解**：根据解析结果选择最合适的生成模型并细化任务参数。例如，针对动作生成任务选用 AnimationGPT，音乐任务选用新的 MusicGen，场景任务选用 Stable Diffusion 全景或 WorldGen。必要时可以进一步拆解任务顺序：有些任务可能相互独立可并行，有些则需要顺序执行（例如先生成动作再根据动作节奏生成音乐）。模型选择逻辑可以配置在适配器中，也可以通过模型登记机制（如维护一个模型功能清单）自动匹配。
3. **模型执行调度（并发支持）**：按照任务列表调度各生成模型执行。适配器应支持并行和串行两种模式：对于互不依赖的任务（如独立的场景图像和音乐生成），可并行启动以缩短总体等待时间；对于有依赖关系的任务，则按顺序等待前一任务完成后再触发下一任务。实现上，可使用多线程/多进程或异步协程来并发执行任务。需要考虑GPU等硬件资源的互斥：例如多个模型共享同一GPU时，适当限制同时间的并发数量以防显存不足。调度过程中，适配器应跟踪每个子任务的执行状态（排队、进行中、已完成），并能够将**进度更新**发送给前端，用于实时反馈任务状态。
4. **缓存查询与状态跟踪**：为提高性能，适配器在调度任务前可检查**缓存**是否已有相同请求的结果。设计一个缓存机制，根据用户输入的关键信息（模型类型+参数哈希等）查询已生成的文件或数据。如果命中缓存且用户未要求强制重新生成，可直接跳过模型执行，返回缓存结果。缓存文件可存储于本地磁盘，并采用明确的命名策略（例如按模型/日期/项目分类存放）。同时，适配器需跟踪当前所有运行任务的状态和结果引用，例如为每个任务分配唯一ID，便于前端查询状态或取消任务。状态跟踪表应记录任务开始时间、所用模型、进度百分比（如可用）以及输出文件路径等信息，以支持断点续传或结果复用。
5. **结果整合与输出**：当所有子任务完成后，适配器收集各部分结果进行整理封装，然后传回前端。整理过程包括：将不同模型输出的文件路径或数据打包成统一的响应对象；如果有需要，还可在服务器端先进行简单合成，例如将场景图像、角色动画和音乐组合打包成一个预览视频（可选步骤，具体取决于实时性和实现复杂度）。适配器应确保输出格式对前端友好，例如提供JSON结构，内含各结果文件的URL或标识，以及任何附加的元数据（如动画时长、帧率，以便前端正确预览）。经过这样的协调流程，中继适配器实现了**“一条指令，多模型协同”**：既隐蔽了后台各模型的复杂调用过程，又保证了对话式创作需求能够被拆解执行并最终汇总反馈给用户。

## 1) “中继适配器”怎么设计：我建议把它正式命名为「生成编排中枢 Orchestrator」

你们描述的“像适配器一样的中间模块”，本质上不是单纯 Adapter，而是：

- **Planner（理解/拆分/路由）**
- **Workflow Orchestrator（任务编排/并发/状态）**
- **Model Gateway（统一输入输出规范 + 模型适配层）**

所以我建议名字就叫：

- 中文：**生成编排中枢** / **任务编排中枢**
- 英文：**Orchestrator** 或 **Generation Orchestrator**
- 代码目录：`orchestrator/` 或 `agent_hub/`

这会比“adapter”更贴近你们未来要加的并发、缓存、状态跟踪、任务队列等能力。

------

## 2) 编排中枢的“最小可用功能集合”（MVP）

你们要求：并发请求处理、理解需求并拆分、选择模型；以后再加缓存/状态跟踪。我的建议是：**MVP 就把“状态跟踪”做成必有**（否则 UI 很难做“进度提示”），缓存可后置。

### 2.1 外部 API（给前端用）

建议你们后端对前端暴露 4 个核心接口即可：

1）**创建任务**

- `POST /api/jobs`
- body：`{ prompt, options }`
- return：`{ job_id }`

2）**查询任务状态**

- `GET /api/jobs/{job_id}`
- return：`{ status, progress, stage, eta?, logs_tail?, assets? }`

3）**订阅进度（推荐 SSE 或 WebSocket）**

- `GET /api/jobs/{job_id}/events`（SSE）
  或 `WS /api/ws/jobs/{job_id}`（WebSocket）

4）**下载/访问资产**

- 静态：`/assets/{job_id}/...`
- 或 `GET /api/assets/{asset_id}` 返回 signed URL / path

> 你们现在 MusicGPT 的输出已经通过 `/musicdata/<filename>` 暴露给前端，这个模式可以统一升级为 `/assets/...`，并保留旧路由兼容。

------

### 2.2 任务状态机（强烈建议你们从一开始就做）

状态建议：

- `QUEUED`（排队）
- `PLANNING`（理解/拆分）
- `RUNNING_MOTION`
- `RUNNING_SCENE`
- `RUNNING_MUSIC`
- `COMPOSING_PREVIEW`（组装预览）
- `EXPORTING_VIDEO`（导出视频）
- `DONE` / `FAILED` / `CANCELED`

每个阶段都要能产出：

- `progress`（0~1 或 0~100）
- `message`（“动作生成中…”）
- `partial_assets`（比如音乐先出来就能播放）

这样前端才能做“像 ChatGPT 一样不断反馈进度”。

------

### 2.3 统一输入输出：定义一个「生成任务中间表示 UIR」

你们目前最大的问题是“网站输入 → 模型输入格式千差万别”。解决它的关键就是：
**建立一个统一的中间 JSON（UIR：Unified Intent Representation）**，编排中枢内部永远只处理 UIR，模型适配器负责 UIR→模型参数。

建议 UIR 结构（示例）：

```json
{
  "project": { "title": "xxx", "created_at": 0 },
  "input": { "raw_prompt": "...", "lang": "zh" },

  "intent": {
    "duration_s": 12,
    "style": "cinematic",
    "mood": "epic",
    "targets": ["scene", "motion", "music"]
  },

  "scene": {
    "provider": "diffusion360",
    "prompt": "A futuristic dojo at night, 360 panorama...",
    "negative": "blurry, seams, watermark",
    "resolution": [2048, 1024]
  },

  "motion": {
    "provider": "animationgpt",
    "prompt": "martial arts combo, fast punch and kick",
    "fps": 30,
    "length_s": 12
  },

  "music": {
    "provider": "musicgpt",
    "prompt": "epic cinematic taiko + synth, 120 bpm",
    "secs": 12
  },

  "output": {
    "need_preview": true,
    "need_export_video": true,
    "export_preset": "1080p"
  }
}
```

UIR 的好处：

- 你们以后替换模型，只改 `scene.provider` 适配器，不动前端、不动 orchestrator 的其他逻辑。
- 你们以后加缓存，只要对 “UIR + model_version” 做 hash。
- 你们以后做作品管理，只要存 UIR + assets manifest。

------

## 3) 模型适配器（Adapter）接口定义：统一调用的核心

建议每个模型都实现一个统一接口（Python 伪代码）：

```py
class ModelAdapter(Protocol):
    name: str
    modality: Literal["scene", "motion", "music", "compose", "export"]
    max_concurrency: int  # GPU 任务建议 1

    def validate(self, uir: dict) -> None: ...
    def build_inputs(self, uir: dict) -> dict: ...
    def run(self, inputs: dict, out_dir: Path, reporter: ProgressReporter) -> dict: ...
```

- `reporter` 用来把日志/阶段进度推给 SSE/WS（即使模型本身没有进度，也能“模拟/分段”进度）。
- `run()` 的返回必须是一个“标准化输出”（比如生成的文件路径、时长、分辨率、可预览 URL 等）。

### 3.1 你们三大核心适配器怎么写（具体到落地）

**A) AnimationGPTAdapter（已存在模型）**

- 输入：`motion.prompt`、`length_s`、`fps` 或动作参数
- 输出：`motion.bvh`（路径 + 帧数 + fps）

**B) MusicGPTCliAdapter（你们现有 exe）**

- 输入：`music.prompt`（中英自动翻译逻辑你们已有）+ `--secs` + `--output`
- 输出：`music.wav`（路径 + secs）

**C) Diffusion360Adapter（你们新增场景模型）**

- 输入：`scene.prompt` + `scene.resolution` + `upscale`
- 调用方式：直接按它 README 的 pipeline 调用并保存 PNG。([GitHub](https://github.com/ArcherFMY/SD-T2I-360PanoImage))
- 输出：`scene_panorama.png`（也可追加生成 cubemap，用于 Three.js 更好渲染）

> 这三者适配后，你们的“综合智能体”就至少能做到：
> **文本 →（分解）→ 生成动作 BVH + 生成 BGM wav + 生成 skybox 全景图**，并把三者在前端统一展示。

------

## 4) 并发与队列：既要“同时来请求”，又不能把 GPU 打爆

你们说希望并发请求处理。**现实中 GPU 推理一般不能真并行**（尤其单卡），但你可以做到：

- API 层并发接入
- Worker 层排队执行
- 前端实时看到队列与进度

建议架构两种可选：

### 4.1 MVP 简化版（我更推荐你们先这样做）

- FastAPI/后端进程里维护：
  - 一个 `asyncio.Queue` 作任务队列
  - 一个 `asyncio.Semaphore(1)` 控制 GPU 同时只有 1 个任务跑
- 任务执行用 `asyncio.to_thread()` 或 `ProcessPoolExecutor` 跑阻塞型脚本/子进程

优点：依赖少，最快跑通。
缺点：重启服务任务会丢（可接受，后续再上 Redis）。

### 4.2 生产版（你们后期要上）

- Redis + Celery/RQ
- 多个 worker：
  - `gpu_worker`（concurrency=1）
  - `cpu_worker`（concurrency=n）
- Job 状态存 DB（SQLite/Postgres）

这更贴近你们文档里“异步处理/任务编排”的方向。

------

## 5) 缓存与作品管理：建议你们把“作品 Manifest”作为一等公民

即使缓存功能你们说可以后加，我也建议你们现在就把输出组织成“manifest.json”，因为作品管理/预览/导出都离不开它：

`/assets/{job_id}/manifest.json` 示例：

```json
{
  "job_id": "xxx",
  "created_at": 0,
  "uir_hash": "sha256:...",
  "inputs": { "raw_prompt": "...", "style": "cinematic" },
  "outputs": {
    "scene": { "panorama_png": "/assets/xxx/scene.png" },
    "motion": { "bvh": "/assets/xxx/motion.bvh", "fps": 30 },
    "music": { "wav": "/assets/xxx/music.wav", "secs": 12 },
    "video": { "mp4": "/assets/xxx/final.mp4" }
  }
}
```

这样：

- 作品列表页只要扫 manifest 就能渲染卡片；
- 预览页只要读 manifest 就能加载资源；
- 导出按钮只要读 manifest 就能提供下载。